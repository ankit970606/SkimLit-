# -*- coding: utf-8 -*-
"""SkimLit ðŸ“„ðŸ”¥

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pqNWfbxsO6GloQ8jWQFxfIysg01_dbfu
"""

# ================================
# PUBMED RCT â€“ MODEL 5 (TOKEN + CHAR + POSITION)
# CLEAN, FIXED, SINGLE-BLOCK CODE
# ================================

# ----------- Imports -----------
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.layers import Layer, TextVectorization
import tensorflow_hub as hub
import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder

# Disable XLA (REQUIRED for string models)
tf.config.optimizer.set_jit(False)

# ----------- Download Dataset -----------
!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git

data_dir = "pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/"

# ----------- Utility Functions -----------
def get_lines(filename):
    with open(filename, "r") as f:
        return f.readlines()

def preprocess_text_with_line_numbers(filename):
    input_lines = get_lines(filename)
    abstract_lines = ""
    abstract_samples = []

    for line in input_lines:
        if line.startswith("###"):
            abstract_lines = ""
        elif line.isspace():
            abstract_split = abstract_lines.splitlines()
            for i, abstract_line in enumerate(abstract_split):
                target, text = abstract_line.split("\t")
                abstract_samples.append({
                    "target": target,
                    "text": text.lower(),
                    "line_number": i,
                    "total_lines": len(abstract_split) - 1
                })
        else:
            abstract_lines += line

    return abstract_samples

# ----------- Load & Prepare Data -----------
train_df = pd.DataFrame(preprocess_text_with_line_numbers(data_dir + "train.txt"))
val_df   = pd.DataFrame(preprocess_text_with_line_numbers(data_dir + "dev.txt"))
test_df  = pd.DataFrame(preprocess_text_with_line_numbers(data_dir + "test.txt"))

train_sentences = train_df["text"].tolist()
val_sentences   = val_df["text"].tolist()

# ----------- Encode Labels -----------
encoder = OneHotEncoder(sparse_output=False)
train_labels = encoder.fit_transform(train_df["target"].to_numpy().reshape(-1,1))
val_labels   = encoder.transform(val_df["target"].to_numpy().reshape(-1,1))

# ----------- Positional Encodings -----------
train_line_numbers = tf.one_hot(train_df["line_number"], depth=15)
val_line_numbers   = tf.one_hot(val_df["line_number"], depth=15)

train_total_lines = tf.one_hot(train_df["total_lines"], depth=20)
val_total_lines   = tf.one_hot(val_df["total_lines"], depth=20)

# ----------- Character-Level Processing -----------
alphabet = "abcdefghijklmnopqrstuvwxyz"
NUM_CHAR_TOKENS = len(alphabet) + 2
output_seq_char_len = 120

char_vectorizer = TextVectorization(
    max_tokens=NUM_CHAR_TOKENS,
    output_sequence_length=output_seq_char_len,
    standardize="lower_and_strip_punctuation"
)

char_vectorizer.adapt(train_sentences)

char_embed = layers.Embedding(
    input_dim=NUM_CHAR_TOKENS,
    output_dim=25,
    mask_zero=True
)

def split_chars(text):
    return " ".join(list(text))

train_chars = [split_chars(s) for s in train_sentences]
val_chars   = [split_chars(s) for s in val_sentences]

# ----------- TF Hub USE (SAFE WRAPPER) -----------
tf_hub_embedding_layer = hub.KerasLayer(
    "https://tfhub.dev/google/universal-sentence-encoder/4",
    trainable=False
)

class USELayer(Layer):
    def call(self, inputs):
        return tf_hub_embedding_layer(inputs)

# ----------- TOKEN MODEL -----------
token_inputs = layers.Input(shape=(), dtype=tf.string)
token_embeddings = USELayer()(token_inputs)
token_outputs = layers.Dense(128, activation="relu")(token_embeddings)
token_model = tf.keras.Model(token_inputs, token_outputs)

# ----------- CHAR MODEL -----------
char_inputs = layers.Input(shape=(1,), dtype=tf.string)
char_vectors = char_vectorizer(char_inputs)
char_embeddings = char_embed(char_vectors)
char_outputs = layers.Bidirectional(layers.LSTM(32))(char_embeddings)
char_model = tf.keras.Model(char_inputs, char_outputs)

# ----------- LINE NUMBER MODEL -----------
line_inputs = layers.Input(shape=(15,), dtype=tf.int32)
line_outputs = layers.Dense(32, activation="relu")(line_inputs)
line_model = tf.keras.Model(line_inputs, line_outputs)

# ----------- TOTAL LINE MODEL -----------
total_inputs = layers.Input(shape=(20,), dtype=tf.int32)
total_outputs = layers.Dense(32, activation="relu")(total_inputs)
total_model = tf.keras.Model(total_inputs, total_outputs)

# ----------- COMBINE EVERYTHING -----------
combined = layers.Concatenate()([token_model.output, char_model.output])
z = layers.Dense(256, activation="relu")(combined)
z = layers.Dropout(0.5)(z)

z = layers.Concatenate()([line_model.output, total_model.output, z])
outputs = layers.Dense(5, activation="softmax")(z)

model_5 = tf.keras.Model(
    inputs=[line_model.input, total_model.input, token_model.input, char_model.input],
    outputs=outputs
)

# ----------- Compile Model -----------
model_5.compile(
    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2),
    optimizer="adam",
    metrics=["accuracy"],
    run_eagerly=True
)

model_5.summary()

# ----------- Build Datasets -----------
train_dataset = tf.data.Dataset.from_tensor_slices((
    (
        train_line_numbers,     # input 1
        train_total_lines,      # input 2
        train_sentences,        # input 3
        train_chars             # input 4
    ),
    train_labels               # labels
)).batch(32).prefetch(tf.data.AUTOTUNE)


val_dataset = tf.data.Dataset.from_tensor_slices((
    (
        val_line_numbers,
        val_total_lines,
        val_sentences,
        val_chars
    ),
    val_labels
)).batch(32).prefetch(tf.data.AUTOTUNE)


# ----------- Callbacks -----------
callbacks = [
    tf.keras.callbacks.ModelCheckpoint(
        "model_5_best.weights.h5",
        monitor="val_loss",
        save_best_only=True,
        save_weights_only=True
    ),
    tf.keras.callbacks.EarlyStopping(
        monitor="val_loss",
        patience=3,
        restore_best_weights=True
    )
]

train_steps = int(0.1 * (len(train_df) // 32))
val_steps   = int(0.1 * (len(val_df) // 32))


# ----------- Train Model -----------
history_model_5 = model_5.fit(
    train_dataset,
    steps_per_epoch=train_steps,
    epochs=50,
    validation_data=val_dataset,
    validation_steps=val_steps,
    callbacks=callbacks
)

model_5.save("model_5.keras")

from google.colab import files
files.download("model_5.keras")

